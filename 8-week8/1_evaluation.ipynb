{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation in Machine Learning\n",
    "\n",
    "A critical step in an ML workflow is evaluating your model. There are two important considerations when evaluating a classifier:\n",
    "\n",
    "1. What measurements to use (these depend on whether we're doing regression or classification)\n",
    "2. How to sample your data for testing \n",
    "\n",
    "We'll talk about both of these things below, but first a brief discussion about the types of errors you can encounter.\n",
    "\n",
    "### Training Error vs. Generalization Error\n",
    "\n",
    "![](assets/IST707-Week21.jpg)\n",
    "\n",
    "In machine learning, evaluating a model's performance involves understanding two key concepts: training error and generalization error. **Training error** refers to the error that the model makes on the training data, the same data it learns from. It's a measure of how well the model fits the training data. However, fitting the training data too closely can lead to overfitting, where the model captures noise along with the underlying data pattern. This is where **generalization error** comes into play. Generalization error measures how well a model performs on unseen data, that is, data not used during the training process. It's an indicator of how well the model has learned to generalize from the training data to broader, unseen instances. A model that performs well on training data but poorly on unseen data (high generalization error) is overfitted, whereas a model that achieves a balance, performing well both on training and unseen data, is considered well-generalized.\n",
    "\n",
    "#### Example\n",
    "Consider a model trained to predict house prices. If this model is trained on a specific dataset of house prices in a city and achieves very low error rates on this training set, it has a low training error. However, if this model, when used to predict prices of houses in a different city (unseen data), performs poorly, it has a high generalization error. The goal in model development is to minimize both training error and generalization error to create a model that is accurate and robust against unseen data.\n",
    "\n",
    "\n",
    "## **1. Types of Measures**\n",
    "\n",
    "### **Regression**\n",
    "\n",
    "When evaluating regression models, three commonly used metrics are R² (R-squared), RMSE (Root Mean Square Error), and MAE (Mean Absolute Error). Let's explore these metrics in detail and see how to implement them in Python.\n",
    "\n",
    "#### R² (R-squared) Score\n",
    "\n",
    "R², also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It provides an indication of the goodness of fit of a model.\n",
    "\n",
    "- R² ranges from 0 to 1.\n",
    "- An R² of 0 indicates that the model explains none of the variability of the data.\n",
    "- An R² of 1 indicates that the model explains all the variability of the data.\n",
    "\n",
    "##### Formula\n",
    "\n",
    "$$ R^2 = 1 - \\frac{SSR}{SST} $$\n",
    "Where:\n",
    "\n",
    "SSR is the sum of squared residuals\n",
    "SST is the total sum of squares\n",
    "\n",
    "More specifically:\n",
    "$$ R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}i)^2}{\\sum{i=1}^n (y_i - \\bar{y})^2} $$\n",
    "Where:\n",
    "\n",
    "$y_i$ are the observed values\n",
    "$\\hat{y}_i$ are the predicted values\n",
    "$\\bar{y}$ is the mean of the observed data\n",
    "\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "- An R² of 0.7 means that 70% of the variance in the target variable can be explained by the model.\n",
    "- Higher R² values indicate a better fit, but be cautious of overfitting when R² is very close to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### RMSE (Root Mean Square Error)\n",
    "\n",
    "RMSE (which we've discussed before) is a frequently used measure of the differences between values predicted by a model and the values actually observed. It represents the standard deviation of the residuals (prediction errors).\n",
    "\n",
    "- RMSE is always non-negative, and a value of 0 indicates a perfect fit to the data.\n",
    "- It has the same units as the dependent variable.\n",
    "- Lower values of RMSE indicate better fit.\n",
    "\n",
    "#### Formula\n",
    "\n",
    "The formula for RMSE is:\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2} $$\n",
    "Where:\n",
    "\n",
    "$n$ is the number of observations\n",
    "$y_i$ are the observed values\n",
    "$\\hat{y}_i$ are the predicted values\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- RMSE can be interpreted as the average deviation of the predictions from the observed values.\n",
    "- It gives more weight to large errors due to the squaring operation.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "Mean Absolute Error (MAE) is another common metric used to evaluate regression models. It measures the average magnitude of the errors in a set of predictions, without considering their direction. \n",
    "\n",
    "#### Formula\n",
    "\n",
    "The formula for MAE is:\n",
    "\n",
    "$$ MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| $$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of observations\n",
    "- $y_i$ are the observed values\n",
    "- $\\hat{y}_i$ are the predicted values\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- MAE is always non-negative, and a value of 0 indicates a perfect fit to the data.\n",
    "- It has the same units as the dependent variable.\n",
    "- Lower values of MAE indicate better fit.\n",
    "- MAE represents the average absolute difference between predicted and actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Comparing MAE and RMSE\n",
    "\n",
    "Both MAE and RMSE are commonly used metrics for regression problems, but they have some key differences:\n",
    "\n",
    "1. **Interpretation**: \n",
    "   - MAE is easier to interpret as it's in the same units as the target variable and represents the average absolute error.\n",
    "   - RMSE is in the same units as the target variable, but it represents the standard deviation of the residuals.\n",
    "\n",
    "2. **Sensitivity to outliers**:\n",
    "   - MAE is less sensitive to outliers because it doesn't square the errors.\n",
    "   - RMSE gives higher weight to large errors due to the squaring operation, making it more sensitive to outliers.\n",
    "\n",
    "3. **Mathematical properties**:\n",
    "   - MAE is based on the L1 norm (sum of absolute values).\n",
    "   - RMSE is based on the L2 norm (sum of squared values).\n",
    "\n",
    "4. **Formula comparison**:\n",
    "   MAE: $\\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$\n",
    "   RMSE: $\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$\n",
    "\n",
    "5. **Use cases**:\n",
    "   - MAE is preferred when you want to treat all errors equally.\n",
    "   - RMSE is preferred when large errors are particularly undesirable, as it penalizes them more heavily.\n",
    "\n",
    "\n",
    "#### Choosing Between MAE and RMSE\n",
    "\n",
    "- Use MAE when you want to treat all errors equally and when outliers are not particularly problematic for your application.\n",
    "- Use RMSE when large errors are especially undesirable, or when you want to maintain mathematical properties like differentiability (RMSE is differentiable everywhere, while MAE is not differentiable at 0).\n",
    "- Often, it's beneficial to report both metrics to provide a more comprehensive view of your model's performance.\n",
    "\n",
    "Remember, the choice between MAE and RMSE (or using both) can depend on your specific problem, the nature of your data, and the requirements of your stakeholders.\n",
    "\n",
    "\n",
    "## Using sklearn\n",
    "\n",
    "Scikit-learn provides easy to use implementations most common metrics in the \"metrics\" package of the library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "# R² score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Classification Problems**\n",
    "\n",
    "**Accuracy** is a commonly used metric in classification problems which calculates the proportion of correct predictions out of all predictions made. It's defined as the number of correct predictions (both true positives and true negatives) divided by the total number of predictions.\n",
    "\n",
    "However, accuracy can be misleading, especially in cases where the dataset is imbalanced, meaning there's a significant difference in the number of instances of the different classes.\n",
    "\n",
    "#### Limitations of Accuracy\n",
    "1. **Does Not Reflect Class Imbalance**: Accuracy does not take into account the imbalance in the distribution of the classes. In a highly imbalanced dataset, even a naive model predicting only the majority class can yield a high accuracy.\n",
    "2. **No Insight into Type of Errors**: Accuracy does not distinguish between the types of errors (false positives vs. false negatives), which can be critical in certain domains like medical diagnosis or fraud detection.\n",
    "\n",
    "#### Example: Breast Cancer Testing\n",
    "Consider a breast cancer screening test applied to 1000 individuals with an accuracy of 94.5%, and the base rate of breast cancer in this population is 5%.\n",
    "\n",
    "- Out of 1000 individuals, 50 (5%) have breast cancer, and 950 (95%) do not.\n",
    "- With an accuracy of 94.5%, the test correctly identifies 945 individuals (both cancer and non-cancer cases).\n",
    "\n",
    "Let's break down this accuracy:\n",
    "\n",
    "1. **True Positives (TP)**: These are individuals who have breast cancer and are correctly identified by the test as having it.\n",
    "2. **True Negatives (TN)**: These are individuals who do not have breast cancer and are correctly identified by the test as not having it.\n",
    "3. **False Positives (FP)**: These are individuals who do not have breast cancer but are incorrectly identified by the test as having it.\n",
    "4. **False Negatives (FN)**: These are individuals who have breast cancer but are incorrectly identified by the test as not having it.\n",
    "\n",
    "Given the accuracy and base rate, we can't directly deduce the exact number of TP, FP, TN, and FN, but we can infer the following:\n",
    "\n",
    "- If all 50 actual cancer cases are correctly identified, then we have 50 TPs. The remaining 895 correct predictions must be TNs (945 correct predictions minus 50 TPs). This would leave 55 incorrect predictions (1000 total minus 945 correct predictions), which would all have to be FPs since all the TPs are accounted for. This scenario implies a very high false positive rate.\n",
    "- Conversely, if some of the cancer cases are missed (FNs), the number of FPs would decrease, but this would increase the FN rate, which is also problematic.\n",
    "\n",
    "Thus, even with a high accuracy of 94.5%, the low base rate of cancer significantly affects the interpretation of the result. A high number of false positives or false negatives can still occur, which can be critical in a medical context. This example illustrates why accuracy alone, especially in the context of imbalanced datasets, might not be a sufficient metric for evaluating the performance of a diagnostic test. Other metrics like precision, recall, and the F1 score are crucial for a more complete evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we've discussed, accuracy is limited in several ways.  There are many other means for evaluating performance, and what is most meaningful depends on the domain problem.  Scikit learn has a robust set of metrics, and I encourage you to read through the documentation hosted at https://scikit-learn.org/stable/modules/model_evaluation.html.\n",
    "\n",
    "In the following, we'll cover some of the more common approaches as well as there implementation in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrices\n",
    "\n",
    "A  _confusion matrix_  is a table layout that allows visualization of the performance of an algorithm.  Though not as convenient as a single number, confusion matrices provides a great deal of information, especially in the case of binary classifiers.  The cells of a confusion matrix include the following values:\n",
    "\n",
    "__True Positives \\(TP\\)__ : These are the correctly predicted positive observations\\.\n",
    "\n",
    "__True Negatives \\(TN\\)__ : These are the correctly predicted negative observations\\.\n",
    "\n",
    "__False Positives \\(FP\\)__ : Incorrectly predicted positive observations \\(Type I error\\)\\.\n",
    "\n",
    "__False Negatives \\(FN\\)__ : Incorrectly predicted negative observations \\(Type II error\\)\\.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Considers and online store that sells video games, and a model that predicts whether a visitor will buy a game or not.\n",
    "\n",
    "|   | predictions |  |  |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| Ground Truth | buy_game = yes | buy_game = no | total |\n",
    "| buy_game = yes | 6700 (TP) | 300 (FN) | 7000 |\n",
    "| buy_game = no | 900 (FP) | 100 (TN) | 1000 |\n",
    "| total | 7600 | 400 | 8000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, we can easily run a confusion matrix as follows (using our digit data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You've hopefully already found the sklearn metrics library\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = digit_data[0]\n",
    "y = digit_data[1]\n",
    "\n",
    "\n",
    "clf = SGDClassifier()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test) \n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, Recall, and F1 Score\n",
    "\n",
    "Precision, Recall, and the F1-Score are alternative metrics used to evaluate the performance of classification models, especially in scenarios where classes are imbalanced. Understanding these metrics is key to interpreting the effectiveness of a model beyond just accuracy.\n",
    "\n",
    "##### Precision\n",
    "- **What It Measures**: Precision quantifies the accuracy of the model in predicting positive instances. It's the ratio of true positives (correctly predicted positives) to the total number of predicted positives (both true positives and false positives).\n",
    "- **Formula**: Precision = True Positives / (True Positives + False Positives)\n",
    "- **Interpretation**: A high precision indicates that the model is reliable in its positive predictions, but it doesn’t tell us how many actual positives were missed.\n",
    "\n",
    "##### Recall (Sensitivity)\n",
    "- **What It Measures**: Recall measures the model's ability to detect positive instances among all actual positives. It's the ratio of true positives to the actual positives in the data (both true positives and false negatives).\n",
    "- **Formula**: Recall = True Positives / (True Positives + False Negatives)\n",
    "- **Interpretation**: High recall means that the model is good at detecting positive instances, but it may also be including some false positives.\n",
    "\n",
    "##### F1-Score\n",
    "- **What It Measures**: The F1-Score is the harmonic mean of precision and recall. It provides a single score that balances both the concerns of precision and recall in one number.\n",
    "- **Formula**: F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "- **Interpretation**: A high F1-Score suggests a balanced trade-off between precision and recall. It is particularly useful when you seek a balance between these two metrics and when the class distribution is imbalanced.\n",
    "\n",
    "![](assets/precision_recall.png)\n",
    "\n",
    "\\* By Walber \\- Own work\\, CC BY\\-SA 4\\.0\\, https://commons\\.wikimedia\\.org/w/index\\.php?curid=36926283\n",
    "\n",
    "##### Example\n",
    "\n",
    "|   | predictions |  |  |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| Ground Truth | buy_game = yes | buy_game = no | total |\n",
    "| buy_game = yes | 6700 (TP) | 300 (FN) | 7000 |\n",
    "| buy_game = no | 900 (FP) | 100 (TN) | 1000 |\n",
    "| total | 7600 | 400 | 8000 |\n",
    "\n",
    "Precision: 6700/7600 = \\.88\n",
    "\n",
    "Recall: 6700 / 7000 = \\.96\n",
    "\n",
    "F1\\-Score:  2\\*\\(\\.96\\+\\.88\\)/\\(\\.96\\*\\.88\\)=\\.92\n",
    "\n",
    "Precision: 100/400 = \\.25\n",
    "\n",
    "Recall: 100 / 1000 = \\.1\n",
    "\n",
    "F1\\-Score:  2\\*\\(\\.25\\+\\.1\\)/\\(\\.25\\*\\.1\\)=\\.14\n",
    "\n",
    "__Average F1 across classes = \\.53__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Precision, Recall and F1\n",
    "\n",
    "Continuing with our digit dataset, sklearn provides metrics that make it easy to calculate precision and recall, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same computation, using the confusion matrix above\n",
    "# TP / (FP + TP)\n",
    "cm[1, 1] / (cm[0, 1] + cm[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP / (FN + TP)\n",
    "cm[1, 1] / (cm[1, 0] + cm[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating by hand\n",
    "cm[1, 1] / (cm[1, 1] + (cm[1, 0] + cm[0, 1]) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curves\n",
    "\n",
    "\n",
    "Many machine learning classifiers, especially binary classifiers, work by computing a decision score or probability for each instance. This score indicates the likelihood of an instance belonging to a particular class. To make a final classification decision (e.g., class 0 or class 1), a threshold is set. Instances with scores above this threshold are assigned to one class, while those below are assigned to the other.\n",
    "\n",
    "For example, in a logistic regression classifier, a decision score is calculated for each instance, often in the form of a probability. By default, a threshold of 0.5 is typically used: instances with probabilities above 0.5 are classified as the positive class, and those below as the negative class.\n",
    "\n",
    "##### The Precision-Recall Tradeoff\n",
    "The choice of threshold has a direct impact on precision and recall, leading to their tradeoff:\n",
    "\n",
    "- **Lowering the Threshold**: This increases recall but can decrease precision. By lowering the threshold, more instances are classified as positive, which means more actual positives are correctly identified (higher recall). However, this also leads to more false positives (lower precision).\n",
    "  \n",
    "- **Raising the Threshold**: Conversely, increasing the threshold boosts precision but can lower recall. A higher threshold means that only instances with a high likelihood are classified as positive, leading to fewer false positives (higher precision). However, this might result in missing out on actual positives (lower recall).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing the tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"decision_function\" method returns the raw value, of the predictor, which is then \n",
    "# thresholded to achieve an outcome\n",
    "\n",
    "y_scores = digit_clf.decision_function([some_digit])\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "# We can plug this directly into cross_val_predict to get the scores across all of our data\n",
    "y_scores = cross_val_predict(digit_clf, X, y, cv=3,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn gives us a really nice way to look at this!\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "threshold = 3000\n",
    "precisions, recalls, thresholds = precision_recall_curve(y, y_scores)\n",
    "plt.figure(figsize=(8, 4))  # extra code – it's not needed, just formatting\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "\n",
    "idx = (thresholds >= threshold).argmax()  # first index ≥ threshold\n",
    "plt.plot(thresholds[idx], precisions[idx], \"bo\")\n",
    "plt.plot(thresholds[idx], recalls[idx], \"go\")\n",
    "plt.axis([-50000, 50000, 0, 1])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"center right\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can graph these two together:\n",
    "\n",
    "import matplotlib.patches as patches  # extra code – for the curved arrow\n",
    "\n",
    "plt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n",
    "\n",
    "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n",
    "\n",
    "plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n",
    "plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\n",
    "plt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n",
    "         label=\"Point at threshold 3,000\")\n",
    "plt.gca().add_patch(patches.FancyArrowPatch(\n",
    "    (0.79, 0.60), (0.61, 0.78),\n",
    "    connectionstyle=\"arc3,rad=.2\",\n",
    "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
    "    color=\"#444444\"))\n",
    "plt.text(0.56, 0.62, \"Higher\\nthreshold\", color=\"#333333\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid()\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this analysis, we can arbitrarily obtain a threshold to achieve a given level of precision or recall:\n",
    "\n",
    "idx_for_90_precision = (precisions >= 0.90).argmax()\n",
    "threshold_for_90_precision = thresholds[idx_for_90_precision]\n",
    "threshold_for_90_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Balancing the Tradeoff\n",
    "- **Context-Dependent**: The optimal balance between precision and recall is highly dependent on the specific context or application. For instance, in spam email detection (where false positives are more tolerable than false negatives), a lower threshold might be preferable. In contrast, for medical diagnostics (where missing a positive diagnosis can be critical), a higher threshold might be chosen to ensure high recall.\n",
    "  \n",
    "- **Adjusting the Threshold**: Some classifiers allow manual adjustment of the decision threshold. Experimenting with different thresholds can help in finding the balance that best suits the specific needs and priorities of the task at hand.\n",
    "\n",
    "##### ROC and AUC\n",
    "\n",
    "Receiver Operating Characteristic (ROC) curves and the Area Under the Curve (AUC), are tools that examine the tradeoff between precision and recall. These are particularly useful for evaluating classification models in the presence of class imbalance or when dealing with probabilistic predictions.\n",
    "\n",
    "##### ROC Curves\n",
    "- **What It Is**: Like the precision-recall curve above, the ROC curve is a graphical representation of a classifier's performance across all possible thresholds. It plots two parameters: \n",
    "   - **True Positive Rate (TPR)**, also known as Recall, on the y-axis.\n",
    "   - **False Positive Rate (FPR)** on the x-axis.\n",
    "- **TPR vs. FPR**: TPR (Recall) is calculated as True Positives / (True Positives + False Negatives), and FPR is calculated as False Positives / (False Positives + True Negatives).\n",
    "- **Interpreting the Curve**: The ROC curve shows the tradeoff between sensitivity (or TPR) and specificity (1 - FPR). A higher curve indicates a better performance. An ideal classifier would have a curve that goes straight up the y-axis and then along the x-axis.\n",
    "\n",
    "Sklearn provides tools to simplify visualization of ROC Curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciKit learn also gives us ROC curves for free\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y, y_scores)\n",
    "\n",
    "idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\n",
    "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n",
    "\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
    "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
    "\n",
    "# just beautifies the figure\n",
    "plt.gca().add_patch(patches.FancyArrowPatch(\n",
    "    (0.20, 0.89), (0.07, 0.70),\n",
    "    connectionstyle=\"arc3,rad=.4\",\n",
    "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
    "    color=\"#444444\"))\n",
    "plt.text(0.12, 0.71, \"Higher\\nthreshold\", color=\"#333333\")\n",
    "plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.grid()\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(loc=\"lower right\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above example, we have enough data that the ROC curve appears to be smooth, but this is not always the case.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "\n",
    "# Generate true labels (40% of them are positive)\n",
    "y_true = np.random.choice([0, 1], size=n_samples, p=[0.8, 0.2])\n",
    "\n",
    "# Generate predicted probabilities\n",
    "y_score = np.linspace(0, 1, n_samples)\n",
    "np.random.shuffle(y_score)\n",
    "\n",
    "# Introduce some noise to make it more realistic\n",
    "y_score = y_score + np.random.normal(0, 0.1, n_samples)\n",
    "y_score = np.clip(y_score, 0, 1)\n",
    "\n",
    "# Compute ROC curve and AUC score\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### AUC (Area Under the ROC Curve)\n",
    "AUC provides a single numeric metric to summarize the ROC curve. It represents the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one.\n",
    "- **Interpretation**: \n",
    "   - An AUC of 1 indicates a perfect classifier.\n",
    "   - An AUC of 0.5 suggests a no-skill classifier, equivalent to random guessing.\n",
    "   - AUC values between 0.5 and 1 indicate different levels of classifier performance, with higher values signifying better classification.\n",
    "\n",
    "#### Calculating the AUC\n",
    "\n",
    "For probabilistic classifiers (not all classifiers are probabilistic!), you can use sklearn to calculate the AUC as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = digit_data[0]\n",
    "y = digit_data[1]\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Scale the data; important for Logistic regression\n",
    "    ('classifier', LogisticRegression(max_iter=5000))  # Step 2: Train a logistic regression model\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "# Get the fitted classifier\n",
    "fitted_clf = pipeline.named_steps['classifier']\n",
    "y_pred_prob = fitted_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculating AUC\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **2. Sampling Data**\n",
    "\n",
    "Generally speaking, we don't make final evaluations on our _training set_.  Instead, what we care most about is how well our model generalizes to unseen data. Thus, we usually hold some data in reserve (which we call \"held-out\" or \"test\" data), train on the non-test data (called the \"training data\"), and evaluate on the test data.  The gold standard for this is \"cross-validation,\" which performs this process multiple times and then averages scores across the different held-out samples. However, cross-validation is computationally more intensive, and so sometimes we'll use simpler methods (e.g., a single train test split).\n",
    "\n",
    "The following sections introduce the basic approaches for sampling data for testing purposes.\n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "* __Basic Idea__ :  Separate your data into two sets – one for “training” the model\\, and the other for “testing” the model\n",
    "* __Important Considerations__\n",
    "  * The training set must be  _completely independent_ and  _highly representative_ of the test set to get an unbiased estimate of performance\n",
    "  * Performance can vary depending on the split\\, and a bad\\-split could result in poorer performance\n",
    "* __Examples__\n",
    "  * Binary classification with a rare positive class \\(e\\.g\\.\\, disease detection where only 5% samples are positive\\)\n",
    "  * Time\\-series data where future data points are used in the training\\, and past data points in the training\n",
    "  * Using data from one geographic region in the training data and another region in the test set\n",
    "\n",
    "#### Example\n",
    "\n",
    "The following shows how to use `train_test_split` in sklearn.\n",
    "\n",
    "#### Implementing train/test split in raw Python\n",
    "\n",
    "Let's start by implementing a simple train/test split function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# This initializes the random state for reproduceability \n",
    "np.random.seed(42)\n",
    "\n",
    "def my_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \n",
    "    \n",
    "    n_samples = len(X)\n",
    "    n_test = int(n_samples * test_size)\n",
    "    \n",
    "    # Create random indices\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Example usage\n",
    "X = np.random.rand(100, 1)  # 100 samples, 1 feature\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) * 0.1  # Linear relationship with some noise\n",
    "\n",
    "X_train, X_test, y_train, y_test = my_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this train test split, we can now train and evaluate a predictor.  We'll use sklearn's linear regression for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(f\"RMSE = {mean_squared_error(y_test,y_pred)}\")\n",
    "print(f\"Explained Variance = {r2_score(y_test, y_pred)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that different splits will return different scores because they are randomly chosen.  This is one reason why we use cross validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in range(5):\n",
    "    X_train, X_test, y_train, y_test = my_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(f\"Trial {trial+1}. RMSE = {mean_squared_error(y_test,y_pred)}\")\n",
    "    print(f\"Trial {trial+1}. Explained Variance = {r2_score(y_test, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Using scikit-learn for train/test split\n",
    "\n",
    "The same result can be achieved more concisely with scikit learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "for trial in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(f\"Trial {trial+1}. RMSE = {mean_squared_error(y_test,y_pred)}\")\n",
    "    print(f\"Trial {trial+1}. Explained Variance = {r2_score(y_test, y_pred)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, scikit-learn's implementation is more concise and likely more optimized.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "After observing that a single train-test split can lead to varying results, cross-validation emerges as a solution to gain a more stable and accurate estimate of a model's performance. In cross-validation, the dataset is divided into multiple smaller sets or \"folds\". The model is then trained and evaluated multiple times, with each fold getting a chance to serve as the test set while the remaining parts are used for training. The most common form of this technique is *k-fold cross-validation*, where the data is split into 'k' number of folds. For each iteration, a different fold is used for testing, and the rest for training. The final performance metric is typically the average of the model's performance across all folds.\n",
    "\n",
    "This process helps in mitigating the variability that comes with relying on a single split. It ensures that every data point is used for both training and testing, which makes the evaluation more reliable and less dependent on the particular way the data is split. By using cross-validation, you gain a better understanding of how the model is likely to perform on unseen data, making it a staple technique in the machine learning model evaluation process.\n",
    "\n",
    "* __K\\-Fold Cross validation:__\n",
    "  * Partition data into  _k_  subsets or \"folds\\.\"\n",
    "  * Train on  _k_ −1 of these folds and test on the remaining fold\n",
    "  * Repeat this process  _k_  times\\, average performance metrics\n",
    "* __Leave\\-one\\-out:__\n",
    "  * Extreme cross\\-validation \\- train on all available data\\, holding back just one case for testing\n",
    "  * Computationally very expensive\n",
    "* __Stratified K\\-Fold Cross\\-Validation:__\n",
    "  * Each fold is stratified\n",
    "* __Considerations__ :\n",
    "  * Not appropriate for time\\-series data \\(use time\\-series specific cross\\-validation\\)\n",
    "  * A greater number of folds increases available training data but\n",
    "  * Increases processing time and performance variance\n",
    "  * Reduces representativeness of test samples\n",
    "  * Data leakage if any preprocessing / feature selection is done after splitting but before training\\.  All such operations need to take place on the training set\\.\n",
    "\n",
    "#### Implementing cross-validation in raw Python\n",
    "\n",
    "Let's implement a simple 5-fold cross-validation function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation RMSE scores: [0.008822075421977847, 0.008696439577628211, 0.009098433928105254, 0.012449230417903762, 0.006548494441384299]\n",
      "Mean RMSE: 0.0091\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def five_fold_cross_validation(X, y, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    fold_size = n_samples // 5\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    for i in range(5):\n",
    "        test_start = i * fold_size\n",
    "        test_end = (i + 1) * fold_size if i < 4 else n_samples\n",
    "        \n",
    "        test_indices = indices[test_start:test_end]\n",
    "        train_indices = np.concatenate([indices[:test_start], indices[test_end:]])\n",
    "        \n",
    "        yield X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "X = np.random.rand(100, 1)  # 100 samples, 1 feature\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) * 0.1 \n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = []\n",
    "for X_train, X_test, y_train, y_test in five_fold_cross_validation(X, y, random_state=42):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    cv_scores.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Cross-validation RMSE scores: {cv_scores}\")\n",
    "print(f\"Mean RMSE: {np.mean(cv_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Using scikit-learn for cross-validation\n",
    "\n",
    "Scikit-learn has lots of tools for handling cross-validation.  `cross_val_score` is an easy one liner that handles most of the simple cases for cross validation.  By default, `cross_val_score` uses R-squared rather than RMSE for regression tasks, but we can pass a string to get one of several predefined scores (see the [docs](https://scikit-learn.org/stable/modules/model_evaluation.html)) or even pass a scoring function.  By default, metrics specified as string in `cross_val_score` are written as fitness functions, so that \"good\" values are higher.  Hence, instead of reporting RMSE, we get _negative_ RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation neg RMSE scores: [-0.01229902 -0.00627026 -0.01532026 -0.00656946 -0.00864188]\n",
      "Mean neg RMSE: -0.0098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "print(f\"Cross-validation neg RMSE scores: {cv_scores}\")\n",
    "print(f\"Mean neg RMSE: {np.mean(cv_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `cross_val_score` only reports one measure (passed as the 'scoring' parameter).  If you want to pass more than one measure, you can use the `cross_validate` method. The `cross_validate` API requires a scorer (rather that a simple metric), so we use the `make_scorer` method to turn a metric into a scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time: 0.001 (+/- 0.001)\n",
      "score_time: 0.004 (+/- 0.010)\n",
      "test_R2: 0.966 (+/- 0.028)\n",
      "train_R2: 0.970 (+/- 0.006)\n",
      "test_RMSE: 0.098 (+/- 0.035)\n",
      "train_RMSE: 0.094 (+/- 0.009)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "\n",
    "scoring = {\n",
    "    'R2': make_scorer(lambda y, y_pred: r2_score(y, y_pred)),\n",
    "    'RMSE': make_scorer(lambda y, y_pred: np.sqrt(mean_squared_error(y, y_pred)))\n",
    "}\n",
    "\n",
    "# Using multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=5, scoring=scoring,return_train_score=True)\n",
    "for key, values in scores.items():\n",
    "    print(f\"{key}: {values.mean():.3f} (+/- {values.std() * 2:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **KFold sampler**\n",
    "\n",
    "If you want even more control over your training and testing, you can use the `KFold` class in sklearn.  This works a lot like our python implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize list to store accuracy for each fold\n",
    "rmse_list = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    # Split the data into current train and test set\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # It's a good idea to use a fresh, untrained model each time you run on new data\n",
    "    # The \"clone\" command does that, but simplifies things by copying other parameters\n",
    "\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    # Fit the model\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    rmse_list.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Calculate and print the average accuracy\n",
    "mean_rmse = np.mean(rmse_list)\n",
    "print(f'Average RMSE: {mean_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stratification\n",
    "\n",
    "Stratification is a technique used in data splitting, especially important in classification problems, to ensure that each subset of the data (such as training and testing sets) has a similar distribution of class labels as the original dataset. This is particularly crucial when dealing with imbalanced datasets, where one or more classes are underrepresented compared to others.\n",
    "\n",
    "When performing a train-test split without stratification in an imbalanced dataset, there's a risk that the distribution of classes in the training and testing sets will be significantly different from each other and from the original dataset. This can lead to biased or inaccurate models, as the model may not be trained or evaluated on a representative sample of data.\n",
    "\n",
    "Stratification addresses this issue by dividing the data in a way that maintains the same proportion of each class in both the training and testing sets as found in the original dataset. In Scikit-Learn's `train_test_split` function, this is achieved by setting the `stratify` parameter to the class labels. As a result, stratification leads to more reliable model evaluation and performance metrics, particularly in scenarios of class imbalance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "First, we're going to artificially unbalance scikit learn's built in breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority_class_X = X[y == 0]\n",
    "majority_class_y = y[y == 0]\n",
    "minority_class_X = X[y == 1]\n",
    "minority_class_y = y[y == 1]\n",
    "\n",
    "# Downsample the minority class\n",
    "minority_size = int(0.1 * len(majority_class_y))  # 10% of the majority class size\n",
    "downsampled_minority_X = minority_class_X[:minority_size]\n",
    "downsampled_minority_y = minority_class_y[:minority_size]\n",
    "\n",
    "# Combine the downsampled minority class with the majority class\n",
    "X_downsampled = np.concatenate([majority_class_X, downsampled_minority_X])\n",
    "y_downsampled = np.concatenate([majority_class_y, downsampled_minority_y])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "X_downsampled, y_downsampled = shuffle(X_downsampled, y_downsampled, random_state=42)\n",
    "\n",
    "# Now X_downsampled and y_downsampled have the downsampled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now analyze results with the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Function to perform train_test_split and evaluate the model\n",
    "def evaluate_model(X, y, stratify=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=stratify)\n",
    "    model = SGDClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluating the model without stratification\n",
    "acc_without_stratification = [evaluate_model(X_downsampled, y_downsampled) for _ in range(200)]\n",
    "\n",
    "# Evaluating the model with stratification\n",
    "acc_with_stratification = [evaluate_model(X_downsampled, y_downsampled, stratify=y_downsampled) for _ in range(200)]\n",
    "\n",
    "# Display results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram for accuracies without stratification\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "plt.hist(acc_without_stratification, bins=10, alpha=0.7, color='blue')\n",
    "plt.title('Accuracies without Stratification')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for accuracies with stratification\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "plt.hist(acc_with_stratification, bins=10, alpha=0.7, color='green')\n",
    "plt.title('Accuracies with Stratification')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()  # Adjusts the plots to fit into the figure cleanly\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the preceding example several times, you should find that the accuracies with stratification have lower overall variance than those models trained and tested without stratification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stratified K-Fold Sampling**\n",
    "\n",
    "If we want to run a stratification with cross-validation, we can use the `StratifiedKFold` class.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "clf = SGDClassifier()\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3)  # add shuffle=True if the dataset is not\n",
    "                                       # already shuffled\n",
    "for train_index, test_index in skfolds.split(X, y_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X[train_index]\n",
    "    y_train_folds = y_5[train_index]\n",
    "    X_test_fold = X[test_index]\n",
    "    y_test_fold = y_5[test_index]\n",
    "\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note that if we don't need fine-grained control, we can simply pass the sampler into the `cross_val_score` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipelines and the problem of data leakage**\n",
    "\n",
    "One challenge that can come up with complex ML analysis is *data leakage*. Data leakage occurs when information from outside your training data inappropriately influences your model during training. A common form of leakage happens when you transform or preprocess your entire dataset before splitting it into training and test sets.\n",
    "\n",
    "For example, imagine you have a dataset and you want to standardize your features (making them have mean 0 and standard deviation 1). If you standardize the entire dataset first and then split it into training and test sets, you're actually using information from your test data (its mean and standard deviation) to transform your training data. This is leakage because in a real-world scenario, you wouldn't have access to future data when training your model.\n",
    "\n",
    "The correct approach is to:\n",
    "\n",
    "1. First split your data into training and test sets\n",
    "2. Calculate any transformations using only the training data\n",
    "3. Apply those same transformations to your test data\n",
    "\n",
    "This is why we use pipelines - they help ensure all transformations are properly fit only on training data and then applied to test data, preventing this type of leakage.\n",
    "\n",
    "Note that combining scikit learn's cross validation routines with pipelines simplify the process of data preparation while avoiding data leakage.  \n",
    "\n",
    "As a working example, we'll use the `wine` dataset, which is focused on predicting the quality of wine based on it's chemical makeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create a stratified K-Fold object\n",
    "stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Use cross_val_score with stratified K-Fold\n",
    "clf = SGDClassifier()\n",
    "scores = cross_val_score(clf, X, y_5, cv=stratified_kfold, scoring='accuracy')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine_data = pd.read_csv(url, delimiter=\";\")\n",
    "wine_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll divide wines into `good` quality and `bad` quality by selecting all wines with a quality score of 7 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classification target variable\n",
    "wine_data['quality_label'] = wine_data['quality'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "\n",
    "# Features and Target\n",
    "X = wine_data.drop(['quality', 'quality_label'], axis=1)\n",
    "y = wine_data['quality_label']\n",
    "\n",
    "wine_training_data = X,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the wine dataset features are of different orders of magnitude, so we'll want to scale features before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with StandardScaler and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Scale the data\n",
    "    ('classifier', LogisticRegression())  # Step 2: Train a logistic regression model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `Pipeline` class follows the estimator API, so all we have to do is pass it into `cross_val_score`.  This will take care of running scaling separately on each fold, hence avoiding any data leakage problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Use cross_val_score to get the scores for each fold\n",
    "scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean cross-validation score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Leave One Out evaluation**\n",
    "\n",
    "Finally, sklearn provide a `LeaveOneOut` sampler. Keep in mind that LOO can be computationally expensive for large datasets because it will train a new model for each sample in the dataset. It's generally used for small datasets or for cases where a high-variance estimate is acceptable.  Note, we'll only do this with a sample dataset, because it is computationally expensive to run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "weights=[0.1, 0.9], n_informative=3, n_redundant=0,\n",
    "flip_y=0, n_features=20, n_clusters_per_class=1,\n",
    "n_samples=100, random_state=42)\n",
    "\n",
    "# Create a Leave-One-Out object\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Use cross_val_score with Leave-One-Out\n",
    "clf = LogisticRegression()\n",
    "scores = cross_val_score(clf, X, y, cv=loo, scoring='accuracy')\n",
    "\n",
    "print(f\"Mean Accuracy: {scores.mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
